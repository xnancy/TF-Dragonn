{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import bcolz\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as ps\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%aimport dataset_interval_reader\n",
    "\n",
    "from shared_examples_queue import SharedExamplesQueue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write some dummy test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_SEQ_CHARS = 4\n",
    "SEQ_LEN_CHR = int(1e5)\n",
    "NUM_CHRS = 5\n",
    "NUM_DATASETS = 3\n",
    "\n",
    "NUM_INTERVALS = 500\n",
    "INTERVAL_LENGTH = 1000\n",
    "NUM_TASKS = 6\n",
    "\n",
    "DATA_DIR = 'test-data'\n",
    "FA_DIRS = [os.path.join(DATA_DIR, 'seq-{}'.format(i)) for i in range(NUM_DATASETS)]\n",
    "BW_DIRS = [os.path.join(DATA_DIR, 'bgw-{}'.format(i)) for i in range(NUM_DATASETS)]\n",
    "\n",
    "INTERVALS_FILE = os.path.join(DATA_DIR, 'intervals_file.json')\n",
    "INPUTS_FILE = os.path.join(DATA_DIR, 'inputs_file.json')\n",
    "\n",
    "TASK_NAMES = ['task-name-{}'.format(i) for i in range(NUM_TASKS)]\n",
    "DATASET_NAMES = ['dataset-name-{}'.format(i) for i in range(NUM_DATASETS)]\n",
    "\n",
    "BLOSC_CPARAMS = bcolz.cparams(clevel=5, shuffle=bcolz.SHUFFLE, cname='lz4')\n",
    "\n",
    "if not os.path.isdir(DATA_DIR):\n",
    "    os.mkdir(DATA_DIR)\n",
    "\n",
    "for FA_DIR in FA_DIRS:\n",
    "    if not os.path.isdir(FA_DIR):\n",
    "        os.mkdir(FA_DIR)\n",
    "\n",
    "for BW_DIR in BW_DIRS:\n",
    "    if not os.path.isdir(BW_DIR):\n",
    "        os.mkdir(BW_DIR)\n",
    "\n",
    "def random_fasta_seq():\n",
    "    seq_idxs = np.random.randint(0, NUM_SEQ_CHARS, SEQ_LEN_CHR)\n",
    "    seq_arr = np.zeros((NUM_SEQ_CHARS, SEQ_LEN_CHR))\n",
    "    seq_arr[seq_idxs, np.arange(SEQ_LEN_CHR, dtype=int)] = 1\n",
    "    return seq_arr\n",
    "\n",
    "def random_bw_data():\n",
    "    # Just use low-frequency wave function for now\n",
    "    bw_data = np.sin(np.arange(SEQ_LEN_CHR) / 1e-3)\n",
    "    return bw_data\n",
    "\n",
    "def random_labels():\n",
    "    # Just random labels for now, as ints\n",
    "    labels = np.random.randint(0, 3, size=(NUM_INTERVALS, NUM_TASKS))\n",
    "    return labels\n",
    "\n",
    "def random_intervals():\n",
    "    interval_starts = np.random.randint(0, SEQ_LEN_CHR - INTERVAL_LENGTH, size=NUM_INTERVALS)\n",
    "    interval_ends = interval_starts + INTERVAL_LENGTH\n",
    "    interval_chrs = np.random.randint(0, NUM_CHRS, size=NUM_INTERVALS)\n",
    "    interval_chrs = np.array(list(map(lambda x: 'chr{}'.format(x), interval_chrs)))\n",
    "    intervals = ps.DataFrame([interval_chrs, interval_starts, interval_ends]).T\n",
    "    return intervals\n",
    "    \n",
    "\n",
    "seq_arrs = {'chr{}'.format(i): random_fasta_seq() for i in range(NUM_CHRS)}\n",
    "bw_arrs = {'chr{}'.format(i): random_bw_data() for i in range(NUM_CHRS)}\n",
    "\n",
    "def dump_to_disk(chr_key, arr, base_dir):\n",
    "    target_fname = os.path.join(base_dir, chr_key)\n",
    "    c_arr = bcolz.carray(arr, cparams=BLOSC_CPARAMS, rootdir=target_fname, mode='w')\n",
    "    c_arr.flush()\n",
    "\n",
    "def write_metadata(base_dir):\n",
    "    # Check the first file to get the shape\n",
    "    arr_shape = bcolz.carray(rootdir=os.path.join(base_dir, 'chr0'), mode='r').shape\n",
    "    chr_shapes = {'chr{}'.format(i): arr_shape for i in range(NUM_CHRS)}\n",
    "    metadata = {'type': 'array_bcolz', 'file_shapes': chr_shapes}\n",
    "    with open(os.path.join(base_dir, 'metadata.json'), 'w') as fp:\n",
    "        json.dump(metadata, fp)\n",
    "\n",
    "for FA_DIR in FA_DIRS:\n",
    "    for chr_key, arr in seq_arrs.items():\n",
    "        dump_to_disk(chr_key, arr, FA_DIR)\n",
    "    write_metadata(FA_DIR)\n",
    "\n",
    "for BW_DIR in BW_DIRS:\n",
    "    for chr_key, arr in bw_arrs.items():\n",
    "        dump_to_disk(chr_key, arr, BW_DIR)\n",
    "    write_metadata(BW_DIR)\n",
    "    \n",
    "intervals_file_dict = {'task_names': TASK_NAMES}\n",
    "inputs_file_dict = {}\n",
    "\n",
    "for dataset_idx, dataset_name in enumerate(DATASET_NAMES):\n",
    "    labels_file = os.path.join(DATA_DIR, 'labels{}.npy'.format(dataset_idx))\n",
    "    intervals_file = os.path.join(DATA_DIR, 'intervals{}.bed'.format(dataset_idx))\n",
    "    \n",
    "    labels = random_labels()\n",
    "    np.save(labels_file, labels)\n",
    "    \n",
    "    intervals = random_intervals()\n",
    "    intervals.to_csv(intervals_file, sep='\\t', header=False, index=False)\n",
    "    \n",
    "    intervals_file_dict[dataset_name] = {'regions': intervals_file, 'labels': labels_file}\n",
    "    inputs_file_dict[dataset_name] = {'dnase_data_dir': BW_DIRS[dataset_idx], 'genome_data_dir': FA_DIRS[dataset_idx]}\n",
    "\n",
    "with open(INTERVALS_FILE, 'w') as fp:\n",
    "    json.dump(intervals_file_dict, fp)\n",
    "\n",
    "with open(INPUTS_FILE, 'w') as fp:\n",
    "    json.dump(inputs_file_dict, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the readers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we want to read is in `test-data/`.\n",
    "\n",
    "We just need to use `dataset_interval_reader.get_readers` to create readers for all the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "readers, task_names = dataset_interval_reader.get_readers_and_tasks(INPUTS_FILE, INTERVALS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s.run(tf.global_variables_initializer())\n",
    "s.run(tf.local_variables_initializer())\n",
    "\n",
    "# Note that you *must* start queue runners before fetching any of the dequeues.\n",
    "queue_runner_threads = tf.train.start_queue_runners(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'dataset-name-0': <tensorflow.python.ops.data_flow_ops.FIFOQueue at 0x1147eb790>,\n",
       " u'dataset-name-1': <tensorflow.python.ops.data_flow_ops.FIFOQueue at 0x1147eb990>,\n",
       " u'dataset-name-2': <tensorflow.python.ops.data_flow_ops.FIFOQueue at 0x114d7ea10>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that readers is a dictionary of `examples_queue`s\n",
    "readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data/dnase_data_dir': array([ 0.53336602, -0.7812708 , -1.39517558, -0.77103299,  0.54488111,\n",
       "         1.40082371,  1.04763794, -0.2055531 , -1.26190436, -1.1968528 ,\n",
       "        -0.06733441,  1.13804913,  1.36429572,  0.4133848 , -0.88240653,\n",
       "        -1.38894749, -0.66289222,  0.66028523,  1.42248476,  0.95659727,\n",
       "        -0.32961288, -1.31040084, -1.12733984,  0.05934735,  1.2110225 ,\n",
       "         1.31969142,  0.29024231, -0.97630769, -1.3714211 , -0.54927814,\n",
       "         0.77054733,  1.43288863,  0.85803723, -0.45087314, -1.34822929,\n",
       "        -1.04862738,  0.18570818,  1.27443552,  1.26465476,  0.1649265 ,\n",
       "        -1.06222105, -1.34273708, -0.43110216,  0.87478262,  1.43195248,\n",
       "         0.75274867, -0.56836092, -1.37508595, -0.96134698,  0.31073433,\n",
       "         1.32777917,  1.1996274 ,  0.03844271, -1.13945735, -1.30312538,\n",
       "        -0.30931234,  0.97215509,  1.41968286,  0.64157611, -0.68113375,\n",
       "        -1.39075565, -0.86619872,  0.43342274,  1.37062573,  1.12513101,\n",
       "        -0.08819428, -1.20739698, -1.25290406, -0.18488584,  1.06188345,\n",
       "         1.3961792 ,  0.52541173, -0.78828692, -1.39511251, -0.76394606,\n",
       "         0.55278915,  1.40263152,  1.04176319, -0.21396853, -1.26549482,\n",
       "        -1.19247591, -0.05882088,  1.14324784,  1.36162949,  0.40518719,\n",
       "        -0.8889606 , -1.3881216 , -0.65540934,  0.66787589,  1.4235394 ,\n",
       "         0.95019293, -0.33787099, -1.31328475, -1.12232554,  0.06787121,\n",
       "         1.21559548,  1.316311  ,  0.28186727, -0.98234725, -1.36983907], dtype=float32),\n",
       " 'data/genome_data_dir': array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,\n",
       "          0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "          0.,  0.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "          1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  0.,  0.,  1.,  1.,  1.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,\n",
       "          1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  1.,  1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "          1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "          0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "          1.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "          1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,\n",
       "          1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,\n",
       "          0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,\n",
       "          0.,  0.,  1.,  0.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,\n",
       "          0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  1.,\n",
       "          0.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  0.,  1.,  0.,  0.,\n",
       "          0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,\n",
       "          0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  1.]], dtype=float32),\n",
       " 'intervals/chrom': 'chr0',\n",
       " 'intervals/end': 5621,\n",
       " 'intervals/start': 5521,\n",
       " 'labels': array([1, 2, 0, 2, 0, 1])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fetch from any of the readers like this\n",
    "s.run(readers['dataset-name-0'].dequeue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Or fetch a batch like this\n",
    "_ = s.run(readers['dataset-name-0'].dequeue_many(128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 28.7 ms per loop\n"
     ]
    }
   ],
   "source": [
    "# We can also check how fast it is,\n",
    "# but note that this is slower because the full result is returned to python\n",
    "\n",
    "def fetch():\n",
    "    _ = s.run(readers['dataset-name-1'].dequeue_many(128))\n",
    "\n",
    "%timeit fetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Set up a parallel examples queue\n",
    "This takes the set of readers as input, and provides a shuffled buffer of examples from all datasets in a defined batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shared_queue = SharedExamplesQueue(readers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s.run(tf.global_variables_initializer())\n",
    "s.run(tf.local_variables_initializer())\n",
    "\n",
    "# Note that you *must* start queue runners before fetching.\n",
    "queue_runner_threads = tf.train.start_queue_runners(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data/dnase_data_dir': <tf.Tensor 'shared-examples-queue/shuffle_batch:0' shape=(128, 100) dtype=float32>,\n",
       " 'data/genome_data_dir': <tf.Tensor 'shared-examples-queue/shuffle_batch:1' shape=(128, 4, 100) dtype=float32>,\n",
       " 'dataset': <tf.Tensor 'shared-examples-queue/shuffle_batch:2' shape=(128,) dtype=string>,\n",
       " 'dataset-index': <tf.Tensor 'shared-examples-queue/shuffle_batch:3' shape=(128,) dtype=int32>,\n",
       " 'intervals/chrom': <tf.Tensor 'shared-examples-queue/shuffle_batch:4' shape=(128,) dtype=string>,\n",
       " 'intervals/end': <tf.Tensor 'shared-examples-queue/shuffle_batch:5' shape=(128,) dtype=int64>,\n",
       " 'intervals/start': <tf.Tensor 'shared-examples-queue/shuffle_batch:6' shape=(128,) dtype=int64>,\n",
       " 'labels': <tf.Tensor 'shared-examples-queue/shuffle_batch:7' shape=(128, 6) dtype=int64>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The shared queue provides outputs shuffled across all the datasets:\n",
    "\n",
    "shared_queue.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset-name-2' 'dataset-name-1' 'dataset-name-0' 'dataset-name-2'\n",
      " 'dataset-name-1' 'dataset-name-1' 'dataset-name-0' 'dataset-name-2'\n",
      " 'dataset-name-2' 'dataset-name-0']\n",
      "[2 1 0 2 2 0 2 2 2 0]\n"
     ]
    }
   ],
   "source": [
    "# It also provides the dataset (and the `dataest-index`) that each example came from\n",
    "\n",
    "# This is intended to make it easy to analyze performance by dataset\n",
    "\n",
    "print(shared_queue.outputs['dataset'].eval()[:10])\n",
    "\n",
    "print(shared_queue.outputs['dataset-index'].eval()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tflow]",
   "language": "python",
   "name": "conda-env-tflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
