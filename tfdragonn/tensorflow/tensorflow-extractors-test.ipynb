{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import bcolz\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as ps\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import dataset_interval_reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write some dummy test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_SEQ_CHARS = 4\n",
    "SEQ_LEN_CHR = int(1e5)\n",
    "NUM_CHRS = 5\n",
    "NUM_DATASETS = 3\n",
    "\n",
    "NUM_INTERVALS = 5000\n",
    "INTERVAL_LENGTH = 1000\n",
    "NUM_TASKS = 6\n",
    "\n",
    "DATA_DIR = 'test-data'\n",
    "FA_DIRS = [os.path.join(DATA_DIR, 'seq-{}'.format(i)) for i in range(NUM_DATASETS)]\n",
    "BW_DIRS = [os.path.join(DATA_DIR, 'bgw-{}'.format(i)) for i in range(NUM_DATASETS)]\n",
    "\n",
    "INTERVALS_FILE = os.path.join(DATA_DIR, 'intervals_file.json')\n",
    "INPUTS_FILE = os.path.join(DATA_DIR, 'inputs_file.json')\n",
    "\n",
    "TASK_NAMES = ['task-name-{}'.format(i) for i in range(NUM_TASKS)]\n",
    "DATASET_NAMES = ['dataset-name-{}'.format(i) for i in range(NUM_DATASETS)]\n",
    "\n",
    "BLOSC_CPARAMS = bcolz.cparams(clevel=5, shuffle=bcolz.SHUFFLE, cname='lz4')\n",
    "\n",
    "if not os.path.isdir(DATA_DIR):\n",
    "    os.mkdir(DATA_DIR)\n",
    "\n",
    "for FA_DIR in FA_DIRS:\n",
    "    if not os.path.isdir(FA_DIR):\n",
    "        os.mkdir(FA_DIR)\n",
    "\n",
    "for BW_DIR in BW_DIRS:\n",
    "    if not os.path.isdir(BW_DIR):\n",
    "        os.mkdir(BW_DIR)\n",
    "\n",
    "def random_fasta_seq():\n",
    "    seq_idxs = np.random.randint(0, NUM_SEQ_CHARS, SEQ_LEN_CHR)\n",
    "    seq_arr = np.zeros((NUM_SEQ_CHARS, SEQ_LEN_CHR))\n",
    "    seq_arr[seq_idxs, np.arange(SEQ_LEN_CHR, dtype=int)] = 1\n",
    "    return seq_arr\n",
    "\n",
    "def random_bw_data():\n",
    "    # Just use low-frequency wave function for now\n",
    "    bw_data = np.sin(np.arange(SEQ_LEN_CHR) / 1e-3)\n",
    "    return bw_data\n",
    "\n",
    "def random_labels():\n",
    "    # Just random labels for now, as ints\n",
    "    labels = np.random.randint(0, 3, size=(NUM_INTERVALS, NUM_TASKS))\n",
    "    return labels\n",
    "\n",
    "def random_intervals():\n",
    "    interval_starts = np.random.randint(0, SEQ_LEN_CHR - INTERVAL_LENGTH, size=NUM_INTERVALS)\n",
    "    interval_ends = interval_starts + INTERVAL_LENGTH\n",
    "    interval_chrs = np.random.randint(0, NUM_CHRS, size=NUM_INTERVALS)\n",
    "    interval_chrs = np.array(list(map(lambda x: 'chr{}'.format(x), interval_chrs)))\n",
    "    intervals = ps.DataFrame([interval_chrs, interval_starts, interval_ends]).T\n",
    "    return intervals\n",
    "    \n",
    "\n",
    "seq_arrs = {'chr{}'.format(i): random_fasta_seq() for i in range(NUM_CHRS)}\n",
    "bw_arrs = {'chr{}'.format(i): random_bw_data() for i in range(NUM_CHRS)}\n",
    "\n",
    "def dump_to_disk(chr_key, arr, base_dir):\n",
    "    target_fname = os.path.join(base_dir, chr_key)\n",
    "    c_arr = bcolz.carray(arr, cparams=BLOSC_CPARAMS, rootdir=target_fname, mode='w')\n",
    "    c_arr.flush()\n",
    "\n",
    "def write_metadata(base_dir):\n",
    "    # Check the first file to get the shape\n",
    "    arr_shape = bcolz.carray(rootdir=os.path.join(base_dir, 'chr0'), mode='r').shape\n",
    "    chr_shapes = {'chr{}'.format(i): arr_shape for i in range(NUM_CHRS)}\n",
    "    metadata = {'type': 'array_bcolz', 'file_shapes': chr_shapes}\n",
    "    with open(os.path.join(base_dir, 'metadata.json'), 'w') as fp:\n",
    "        json.dump(metadata, fp)\n",
    "\n",
    "for FA_DIR in FA_DIRS:\n",
    "    for chr_key, arr in seq_arrs.items():\n",
    "        dump_to_disk(chr_key, arr, FA_DIR)\n",
    "    write_metadata(FA_DIR)\n",
    "\n",
    "for BW_DIR in BW_DIRS:\n",
    "    for chr_key, arr in bw_arrs.items():\n",
    "        dump_to_disk(chr_key, arr, BW_DIR)\n",
    "    write_metadata(BW_DIR)\n",
    "    \n",
    "intervals_file_dict = {'task_names': TASK_NAMES}\n",
    "inputs_file_dict = {}\n",
    "\n",
    "for dataset_idx, dataset_name in enumerate(DATASET_NAMES):\n",
    "    labels_file = os.path.join(DATA_DIR, 'labels{}.npy'.format(dataset_idx))\n",
    "    intervals_file = os.path.join(DATA_DIR, 'intervals{}.bed'.format(dataset_idx))\n",
    "    \n",
    "    labels = random_labels()\n",
    "    np.save(labels_file, labels)\n",
    "    \n",
    "    intervals = random_intervals()\n",
    "    intervals.to_csv(intervals_file, sep='\\t', header=False, index=False)\n",
    "    \n",
    "    intervals_file_dict[dataset_name] = {'regions': intervals_file, 'labels': labels_file}\n",
    "    inputs_file_dict[dataset_name] = {'dnase_data_dir': BW_DIRS[dataset_idx], 'genome_data_dir': FA_DIRS[dataset_idx]}\n",
    "\n",
    "with open(INTERVALS_FILE, 'w') as fp:\n",
    "    json.dump(intervals_file_dict, fp)\n",
    "\n",
    "with open(INPUTS_FILE, 'w') as fp:\n",
    "    json.dump(inputs_file_dict, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data we want to read is now in test-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-af39d1cd0198>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_interval_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_readers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUTS_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINTERVALS_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/chris/dev/tf-dragonn/tfdragonn/tensorflow/dataset_interval_reader.py\u001b[0m in \u001b[0;36mget_readers\u001b[0;34m(processed_inputs_file, processed_intervals_file)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         examples_queues[dataset_id] = get_readers_for_dataset(\n\u001b[0;32m---> 41\u001b[0;31m             intervals, datafiles, labels, name=dataset_id)\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mexamples_queues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chris/dev/tf-dragonn/tfdragonn/tensorflow/dataset_interval_reader.py\u001b[0m in \u001b[0;36mget_readers_for_dataset\u001b[0;34m(intervals, datafiles, labels, name, read_batch_size)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Queue to store intervals to read, outputs are dequeued tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         to_read = interval_queue(intervals, labels, dequeue_size=read_batch_size,\n\u001b[0;32m---> 64\u001b[0;31m                                  name='interval-queue')\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Create a reader for each datafile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mread_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chris/dev/tf-dragonn/tfdragonn/tensorflow/interval_queue.py\u001b[0m in \u001b[0;36minterval_queue\u001b[0;34m(intervals, labels, dequeue_size, name)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0mare\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \"\"\"\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mn_exs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'chrom'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'start'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mintervals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list object is not an iterator"
     ]
    }
   ],
   "source": [
    "readers = dataset_interval_reader.get_readers(INPUTS_FILE, INTERVALS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr4\t37070\t38070\r\n",
      "chr3\t64737\t65737\r\n",
      "chr2\t70491\t71491\r\n",
      "chr2\t32290\t33290\r\n",
      "chr1\t59849\t60849\r\n",
      "chr0\t98491\t99491\r\n",
      "chr4\t58996\t59996\r\n",
      "chr4\t29531\t30531\r\n",
      "chr0\t28951\t29951\r\n",
      "chr4\t49048\t50048\r\n"
     ]
    }
   ],
   "source": [
    "! head data2/intervals0.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.randint(0, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.tofile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chris/dev/bcolz-reader-local-dev\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tflow]",
   "language": "python",
   "name": "conda-env-tflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
